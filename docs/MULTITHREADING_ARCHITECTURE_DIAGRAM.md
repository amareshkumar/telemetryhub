# Multithreading Architecture - TelemetryHub Gateway
**For Senior-Level Technical Interviews**

## Quick Reference Card

**Pattern:** Producer-Consumer + Thread Pool  
**Threads:** 8 HTTP + 4 Processing + 2 Core (producer/consumer)  
**Synchronization:** mutex + condition_variable + atomics  
**Performance:** 3,720 req/s @ 1.72ms p95 (100 VUs, 0% errors)

---

## System Overview Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TelemetryHub Gateway                            â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      HTTP Server Layer                         â”‚   â”‚
â”‚  â”‚                   (cpp-httplib - 8 threads)                    â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  GET /status    POST /start    POST /stop    GET /metrics     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚               â”‚                                        â”‚              â”‚
â”‚               â–¼                                        â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                       GatewayCore                              â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚   Producer   â”‚â”€â”€â”€â–¶â”‚ TelemetryQ   â”‚â”€â”€â”€â–¶â”‚   Consumer      â”‚ â”‚   â”‚
â”‚  â”‚  â”‚   Thread     â”‚    â”‚  (bounded)   â”‚    â”‚   Thread        â”‚ â”‚   â”‚
â”‚  â”‚  â”‚              â”‚    â”‚              â”‚    â”‚                 â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ Device I/O   â”‚    â”‚ mutex+cv     â”‚    â”‚ queue.pop()     â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ read_sample()â”‚    â”‚ capacity:    â”‚    â”‚ process()       â”‚ â”‚   â”‚
â”‚  â”‚  â”‚              â”‚    â”‚ 1000         â”‚    â”‚                 â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ 100ms sleep  â”‚    â”‚              â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚   â”‚
â”‚  â”‚  â”‚              â”‚    â”‚ Drop oldest  â”‚    â”‚ â”‚ ThreadPool  â”‚ â”‚ â”‚   â”‚
â”‚  â”‚  â”‚              â”‚    â”‚ on overflow  â”‚    â”‚ â”‚ (4 workers) â”‚ â”‚ â”‚   â”‚
â”‚  â”‚  â”‚              â”‚    â”‚              â”‚    â”‚ â”‚             â”‚ â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚ Job Queue   â”‚ â”‚ â”‚   â”‚
â”‚  â”‚                                           â”‚ â”‚ + Workers[] â”‚ â”‚ â”‚   â”‚
â”‚  â”‚                                           â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚   â”‚
â”‚  â”‚                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  Metrics: samples_processed, samples_dropped, queue_depth     â”‚   â”‚
â”‚  â”‚           pool_jobs_processed, pool_avg_processing_ms         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                       Device Layer                             â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  Device (state machine)     SerialPortSim (UART)              â”‚   â”‚
â”‚  â”‚  Idle â”€â”€â–¶ Measuring â”€â”€â–¶ SafeState/Error                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Detailed Component Diagrams

### 1. TelemetryQueue (Thread-Safe Bounded Queue)

```cpp
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        TelemetryQueue                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Private:                                    â”‚
â”‚   std::queue<TelemetrySample> queue_       â”‚
â”‚   std::mutex mutex_                         â”‚
â”‚   std::condition_variable cv_               â”‚
â”‚   size_t max_size_ = 1000                   â”‚
â”‚   bool shutdown_ = false                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Public API:                                 â”‚
â”‚   void push(TelemetrySample&&)  // Move    â”‚
â”‚   std::optional<Sample> pop()   // Block   â”‚
â”‚   void shutdown()                           â”‚
â”‚   size_t size()                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Push Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Producerâ”‚â”€â”€â”€â”€â–¶â”‚ lock_guard   â”‚â”€â”€â”€â”€â–¶â”‚Queue full?â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ (mutex_)     â”‚     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
                                          â”‚ Yes
                                          â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚ queue_.pop() â”‚  Drop oldest
                                   â”‚ (backpressure)â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚ emplace()    â”‚  New sample
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚cv_.notify_oneâ”‚  Wake consumer
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pop Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Consumerâ”‚â”€â”€â”€â”€â–¶â”‚ unique_lock  â”‚â”€â”€â”€â”€â–¶â”‚ cv_.wait()   â”‚ (sleep)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ (mutex_)     â”‚     â”‚ until data   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â”‚ Data available!
                                           â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚ queue_.front()â”‚
                                    â”‚ queue_.pop() â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚ return sampleâ”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. ThreadPool Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ThreadPool (4 workers)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Job Submission:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚ submit() â”‚â”€â”€â–¶ lock(queue_mutex_)                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                                          â”‚
â”‚                  â–¼                                           â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚           â”‚ jobs_.push(job)â”‚                                â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                  â”‚                                           â”‚
â”‚                  â–¼                                           â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚           â”‚ cv_.notify_one()â”‚  Wake one worker              â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                              â”‚
â”‚  Worker Threads (Pre-created, persistent):                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚Worker #1â”‚  â”‚Worker #2â”‚  â”‚Worker #3â”‚  â”‚Worker #4â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚
â”‚       â”‚            â”‚            â”‚            â”‚              â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚              â”‚   Worker Loop (each)   â”‚                     â”‚
â”‚              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                     â”‚
â”‚              â”‚ 1. unique_lock(mutex)  â”‚                     â”‚
â”‚              â”‚ 2. cv_.wait(has_job)   â”‚  Sleep until work  â”‚
â”‚              â”‚ 3. job = jobs_.front() â”‚                     â”‚
â”‚              â”‚ 4. jobs_.pop()         â”‚                     â”‚
â”‚              â”‚ 5. unlock              â”‚                     â”‚
â”‚              â”‚ 6. auto start = now()  â”‚  Start timer       â”‚
â”‚              â”‚ 7. job()               â”‚  â—€â”€â”€ Execute work  â”‚
â”‚              â”‚ 8. auto end = now()    â”‚  Stop timer        â”‚
â”‚              â”‚ 9. update_metrics()    â”‚  Atomic counters   â”‚
â”‚              â”‚ 10. GOTO 1             â”‚  Loop forever      â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                              â”‚
â”‚  Metrics (Lock-free, atomic):                               â”‚
â”‚    std::atomic<uint64_t> jobs_processed_                    â”‚
â”‚    std::atomic<uint64_t> total_processing_time_us_          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Producer-Consumer Flow

```
Time â”€â”€â–¶

Producer Thread:                    Queue:              Consumer Thread:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”€â”€â”€â”€â”€               â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Device I/O  â”‚
â”‚ read_sample()â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Move sample â”‚
â”‚ to queue    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚Sample1â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                         â”‚
       â”‚ sleep(100ms)            â”‚ cv_.wait()
       â”‚                         â”‚ (sleeping...)
       â–¼                         â”‚
                                 â”‚ cv_.notify_one()
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚ â–¼
â”‚ read_sample()â”‚                 â”‚ WAKE UP!
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                 â”‚
       â”‚                         â”‚
       â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Move sample â”‚            â”‚Sample2â”‚      â”‚ pop() Sample1â”‚
â”‚ to queue    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚       â”‚â—€â”€â”€â”€â”€â”€â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
       â”‚                         â”‚                  â–¼
       â”‚                         â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                         â”‚          â”‚ ThreadPool   â”‚
       â”‚                         â”‚          â”‚ .submit(job) â”‚
       â”‚                         â”‚          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼                         â–¼                 â”‚
                                                   â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚ Worker picks â”‚
                                            â”‚ job, executesâ”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
(REPEAT)                                           â”‚
                                                   â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚ Update metricsâ”‚
                                            â”‚ (atomic)     â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Synchronization Primitives (Interview Gold)

### Primitives Used

| Primitive | Location | Purpose | Performance |
|-----------|----------|---------|-------------|
| **`std::mutex`** | TelemetryQueue, ThreadPool | Mutual exclusion for queue/job access | ~25ns (uncontended) |
| **`std::condition_variable`** | Queue pop, worker sleep | Efficient blocking, no busy-wait | ~1Î¼s wake-up |
| **`std::atomic<bool>`** | `running_`, `stop_` flags | Lock-free state checks | ~5ns |
| **`std::atomic<uint64_t>`** | Metrics counters | Lock-free increment | ~10ns |
| **`std::lock_guard`** | All short critical sections | RAII locking (exception-safe) | Wrapper (0 overhead) |
| **`std::unique_lock`** | Condition variable wait | Allows unlock/relock | Wrapper (0 overhead) |

### Why Condition Variable? (Common Interview Question)

**Question:** *"Why not just busy-wait with `while(queue.empty()) {}`?"*

**Answer:**
```cpp
// âŒ WRONG - Busy-wait (burns CPU)
while (queue_.empty()) {
    // CPU at 100% doing nothing!
    // With 4 workers idle = 400% CPU waste
}

// âœ… RIGHT - Condition variable (efficient sleep)
cv_.wait(lock, [this] { return !queue_.empty() || stop_; });
// CPU usage: ~0% when idle
// Wake-up latency: ~1Î¼s (fast enough for 3,720 req/s)
```

**Result:**
- Idle CPU drops from 400% â†’ 0% (4 worker threads sleeping)
- Latency impact: Negligible (~1Î¼s wake-up vs 10ms processing time)

---

## Thread Lifecycle Diagram

```
Application Startup                 Runtime                   Shutdown
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€                   â”€â”€â”€â”€â”€â”€â”€â”€

main()
  â”‚
  â–¼
GatewayCore()
  â”‚
  â”œâ”€â”€â–¶ ThreadPool(4)
  â”‚     â”‚
  â”‚     â”œâ”€â”€â–¶ workers_[0] = std::thread(&worker_loop, this)  â—€â”€â”
  â”‚     â”œâ”€â”€â–¶ workers_[1] = std::thread(&worker_loop, this)  â—€â”€â”¤ Pre-create
  â”‚     â”œâ”€â”€â–¶ workers_[2] = std::thread(&worker_loop, this)  â—€â”€â”¤ all workers
  â”‚     â””â”€â”€â–¶ workers_[3] = std::thread(&worker_loop, this)  â—€â”€â”˜
  â”‚           â”‚
  â”‚           â””â”€â”€â–¶ All workers sleep in cv_.wait()
  â”‚
  â–¼
start()
  â”‚
  â”œâ”€â”€â–¶ producer_thread_ = std::thread(&producer_loop, this)  â—€â”€ Launch
  â”‚                                                               producer
  â””â”€â”€â–¶ consumer_thread_ = std::thread(&consumer_loop, this)  â—€â”€ Launch
                                                                  consumer

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ THREADS RUNNING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Producer:    Read device â†’ Push queue â†’ sleep(100ms) â†’ REPEAT
Consumer:    Pop queue â†’ Submit to ThreadPool â†’ REPEAT
Workers[0-3]: Wait for jobs â†’ Execute â†’ Update metrics â†’ REPEAT
HTTP[0-7]:    Wait for requests â†’ Handle â†’ Respond â†’ REPEAT

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ stop() called â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

stop()
  â”‚
  â”œâ”€â”€â–¶ running_ = false                    Stop producer/consumer
  â”‚
  â”œâ”€â”€â–¶ queue_.shutdown()                   Wake consumer one last time
  â”‚
  â”œâ”€â”€â–¶ producer_thread_.join()             Wait for producer exit
  â”‚
  â””â”€â”€â–¶ consumer_thread_.join()             Wait for consumer exit

~GatewayCore()  (destructor)
  â”‚
  â””â”€â”€â–¶ ~ThreadPool()
        â”‚
        â”œâ”€â”€â–¶ stop_ = true                  Signal workers to exit
        â”œâ”€â”€â–¶ cv_.notify_all()              Wake all sleeping workers
        â”‚
        â”œâ”€â”€â–¶ workers_[0].join()            Wait for each worker
        â”œâ”€â”€â–¶ workers_[1].join()
        â”œâ”€â”€â–¶ workers_[2].join()
        â””â”€â”€â–¶ workers_[3].join()

All threads cleaned up, resources released âœ…
```

---

## Lock-Free Metrics (Atomic Operations)

### Why Atomic Instead of Mutex?

```cpp
// Metrics updated on EVERY sample (hot path!)

// âŒ WRONG - Mutex on hot path
void update_metrics() {
    std::lock_guard lock(metrics_mutex_);  // 100ns+ contention
    samples_processed_++;
}
// At 3,720 req/s: 3,720 Ã— 100ns = 372Î¼s lost to locking per second

// âœ… RIGHT - Atomic (lock-free)
void update_metrics() {
    samples_processed_.fetch_add(1, std::memory_order_relaxed);  // 10ns
}
// At 3,720 req/s: 3,720 Ã— 10ns = 37Î¼s (10Ã— faster)
```

### Memory Ordering Explained

```cpp
// metrics_samples_processed_.fetch_add(1, std::memory_order_relaxed);
//                                         ^^^^^^^^^^^^^^^^^^^^^^^
//                                         Why relaxed?

// memory_order_relaxed:
//   - No synchronization with other threads
//   - Only guarantees atomic increment
//   - Fastest option (~10ns)
//   - OK for independent counters (order doesn't matter)

// When to use stricter ordering:
// memory_order_acquire/release: Producer-consumer handoff
// memory_order_seq_cst: When you need total ordering (slowest)

// Example: We don't care if Counter A = 10 and Counter B = 5
//          appear in different order to different threads.
//          We only care that final sum is correct.
```

---

## Backpressure Strategy (Bounded Queue)

### The Problem

```
Producer: 10,000 samples/sec
Consumer:  1,000 samples/sec  (10Ã— slower!)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Queue grows unbounded â†’ Out of Memory â†’ CRASH ğŸ’¥
```

### Our Solution: Drop Oldest

```cpp
void TelemetryQueue::push(TelemetrySample&& sample) {
    std::lock_guard lock(mutex_);
    
    if (max_size_ > 0 && queue_.size() >= max_size_) {
        queue_.pop();  // Drop oldest sample (FIFO)
        // samples_dropped_++;  // Track for metrics
    }
    
    queue_.emplace(std::move(sample));
}
```

**Trade-offs:**
- âœ… **Prevents OOM** - Memory bounded (1000 Ã— sizeof(Sample) â‰ˆ 40KB)
- âœ… **Liveness** - System keeps running, never blocks
- âš ï¸ **Data loss** - Oldest samples dropped (acceptable for telemetry)

**Alternatives (interview talking points):**
1. **Block producer** - Waits for space, risks device I/O timeout
2. **Reject new** - Drop incoming, but then we lose *latest* data
3. **Drop oldest** - Our choice, prioritizes recent data âœ…

---

## Performance Validation (Load Testing Results)

### Test Setup
```
Tool: k6 (Grafana)
Duration: 1 minute 38 seconds
Virtual Users: 100 (concurrent connections)
Target: http://localhost:8080/status (GET)
```

### Results

| Metric | Value | Analysis |
|--------|-------|----------|
| **Requests** | 365,781 total | 3,720 req/s average |
| **Success Rate** | 100% (0 errors) | âœ… No dropped requests |
| **p50 Latency** | 0.85ms | Median response time |
| **p95 Latency** | 1.72ms | 95th percentile |
| **p99 Latency** | 2.34ms | 99th percentile |
| **HTTP Threads** | 8 (cpp-httplib) | I/O bound |
| **Worker Threads** | 4 (ThreadPool) | CPU bound |
| **Queue Capacity** | 1000 samples | Backpressure threshold |

### Interview Talking Point

> "Under load testing with 100 concurrent users (k6), the gateway sustained **3,720 requests per second** with **p95 latency of 1.72ms** and **zero errors**. The system uses **12 threads total**: 8 for HTTP I/O and 4 for telemetry processing. The bounded queue (1000 capacity) provides backpressure - if processing can't keep up, we drop oldest samples rather than crashing with OOM. This design prioritizes **liveness over completeness**, which is appropriate for streaming telemetry data."

---

## Common Interview Questions

### Q1: "How do you prevent deadlock?"

**Answer:**
> "I follow two rules: (1) **Lock ordering discipline** - always acquire locks in the same order (never hold TelemetryQueue lock while acquiring ThreadPool lock), and (2) **RAII with `std::lock_guard`** - ensures automatic unlock even if exceptions occur. I also minimize lock scope - critical sections are only around queue operations, not business logic."

### Q2: "What if producer is faster than consumer?"

**Answer:**
> "The bounded queue (1000 capacity) provides **backpressure**. When full, we drop the oldest sample using FIFO policy. This trades **completeness for liveness** - the system keeps running rather than blocking or crashing with OOM. For telemetry data, recent samples are more valuable than old ones, so this is the right trade-off. If we needed guaranteed delivery, I'd add a circuit breaker to return HTTP 503 when overloaded."

### Q3: "Why thread pool instead of spawning threads per request?"

**Answer:**
> "Thread creation is expensive (~1ms per `std::thread` constructor). At 3,720 req/s, that's 3.72 **seconds** of CPU time wasted per second just creating threads! Thread pool amortizes this cost: create 4 threads once at startup, reuse forever. Workers sleep in `cv_.wait()` (~0% CPU idle) and wake up in ~1Î¼s when work arrives."

### Q4: "Why 4 worker threads? Why not 8 or 16?"

**Answer:**
> "Based on profiling, the work is **CPU-bound** (JSON parsing, metric updates). My dev machine has 4 physical cores (8 with hyperthreading). Going beyond 4 threads hits diminishing returns due to context switching overhead. However, this should be **configurable** - in production, I'd make it a config parameter (e.g., `thread_pool_size = std::thread::hardware_concurrency()`)."

### Q5: "What about cache coherence with atomics across threads?"

**Answer:**
> "Atomic operations on x86_64 use the **MESI cache coherence protocol**. When thread 1 does `fetch_add` on a counter, the CPU invalidates that cache line in other cores. With `memory_order_relaxed`, we skip memory fences (faster), accepting that different threads might see slightly stale values temporarily. For counters, this is fine - we only care about eventual consistency when reading metrics. If I needed strict ordering (e.g., publish-subscribe), I'd use `acquire/release` ordering."

---

## Code References

### Key Files
- **ThreadPool:** [`gateway/src/ThreadPool.cpp`](../gateway/src/ThreadPool.cpp)
- **TelemetryQueue:** [`gateway/src/TelemetryQueue.cpp`](../gateway/src/TelemetryQueue.cpp)
- **GatewayCore:** [`gateway/src/GatewayCore.cpp`](../gateway/src/GatewayCore.cpp)
- **HTTP Server:** [`gateway/src/http_server.cpp`](../gateway/src/http_server.cpp)

### Quick Code Snippets for Interviews

**Condition Variable Wait (Efficient Blocking):**
```cpp
// From TelemetryQueue::pop()
std::unique_lock lock(mutex_);
cv_.wait(lock, [this] { return shutdown_ || !queue_.empty(); });
```

**Atomic Metrics (Lock-Free Counters):**
```cpp
// From ThreadPool::worker_loop()
jobs_processed_.fetch_add(1, std::memory_order_relaxed);
total_processing_time_us_.fetch_add(duration_us, std::memory_order_relaxed);
```

**Move Semantics (Zero-Copy):**
```cpp
// From TelemetryQueue::push()
queue_.emplace(std::move(sample));  // No copy, just pointer swap
```

**RAII Locking (Exception-Safe):**
```cpp
// From TelemetryQueue::size()
std::lock_guard lock(mutex_);  // Automatic unlock on scope exit
return queue_.size();
```

---

## Summary: Key Talking Points for Interviews

**30-Second Elevator Pitch:**
> "I built a high-performance telemetry gateway in C++17 using producer-consumer pattern with thread pooling. It handles 3,720 requests per second with p95 latency of 1.72ms under 100 concurrent connections. The architecture uses 8 HTTP threads for I/O, 4 worker threads for CPU-bound processing, bounded queues for backpressure, and lock-free atomic operations for metrics. All validated with k6 load testing showing 100% success rate."

**Architecture Pattern:** Producer-Consumer + Thread Pool  
**Synchronization:** Mutex + Condition Variables + Atomics  
**Performance:** 3,720 req/s @ 1.72ms p95  
**Reliability:** 0% errors under load, bounded queue prevents OOM  
**Modern C++:** Move semantics, RAII, memory_order_relaxed  

---

**Date:** December 31, 2025  
**Version:** 5.0.0-day5-final  
**Load Testing:** Validated with k6 (Grafana), 100 VUs, 98s duration
